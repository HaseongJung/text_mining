{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the libraries for crwaling\n",
    "!pip install requests   # HTTP 요청을 보내는 라이브러리\n",
    "!pip install beautifulsoup4 # HTML을 파싱하는 라이브러리\n",
    "!pip install selenium   # 동적 크롤링을 위한 라이브러리\n",
    "!pip install webdriver_manager  # 크롬 드라이버를 설치하는 라이브러리\n",
    "!pip install pandas  # 데이터를 다루는 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import time\n",
    "import re\n",
    "import random as rd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = \"Mozilla/5.0 (Linux; Android 9; SM-G975F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.83 Mobile Safari/537.36\"\n",
    "headers = {\n",
    "    \"User-Agent\": user_agent\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BS4를 활용한 기사제목 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = \"Mozilla/5.0 (Linux; Android 9; SM-G975F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.83 Mobile Safari/537.36\"\n",
    "headers = {\n",
    "    \"User-Agent\": user_agent\n",
    "}\n",
    "\n",
    "# 키워드를 입력하여 URL 생성\n",
    "query = input(\"검색어를 입력하세요: \")\n",
    "start_date = re.sub(r'[^0-9]', '', input(\"시작 날짜를 yyyy.mm.dd 형식으로 입력하세요: \")) # 시작 날짜(정규식을 이용하여 숫자만 추출)\n",
    "end_date = re.sub(r'[^0-9]', '', input(\"종료 날짜를 yyyy.mm.dd 형식으로 입력하세요: \"))    # 종료 날짜(정규식을 이용하여 숫자만 추출)\n",
    "\n",
    "num_articles_per_page = 15  # 페이지당 기사 수\n",
    "num_pages = int(input(\"크롤링할 페이지 수를 입력하세요: \"))  # 크롤링할 페이지 수\n",
    "\n",
    "# 데이터를 저장할 리스트 초기화\n",
    "title_list = []\n",
    "\n",
    "for page in tqdm(range(num_pages), desc='Pages...'):\n",
    "    start_index = page * num_articles_per_page + 1  # 페이지의 시작 인덱스 계산\n",
    "\n",
    "    # URL 생성\n",
    "    # url = f'https://search.naver.com/search.naver?where=news&sm=tab_jum&query={query}&start={start_index}&pd=3&ds={start_date}&de={end_date}'\n",
    "    url = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query=' + \\\n",
    "        '%s' % query + \"&start=\" + str(start_index) + \"&pd=3&ds=\" + \\\n",
    "        '%s' % start_date + \"&de=\" + '%s' % end_date\n",
    "\n",
    "    print(f'URL: {url}')\n",
    "\n",
    "    # HTML 문서 가져오기\n",
    "    response = requests.get(url, headers=headers);  print(f'status code: {response.status_code}')\n",
    "    html = response.text\n",
    "\n",
    "    # BeautifulSoup을 이용하여 HTML 파싱\n",
    "    soup = bs(html, 'html.parser'); \n",
    "\n",
    "    # 기사 제목 추출\n",
    "    titles = soup.find_all('a', class_=\"news_tit\")\n",
    "\n",
    "    # 추출한 데이터를 리스트에 저장\n",
    "    for title in tqdm(titles, desc=\"Links...\"):\n",
    "        print(title.text)\n",
    "        title_list.append(title.text)\n",
    "        \n",
    "    print(f'{page}번째 페이지')\n",
    "    print(f'기사 갯수: {len(title_list)}')\n",
    "    print(f'{\"-\"*200}')\n",
    "    time.sleep(rd.uniform(2.5, 3.5))\n",
    "        \n",
    "print(f'총 기사 갯수: {len(title_list)}')\n",
    "print(title_list)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df = pd.DataFrame(title_list, columns=[\"뉴스 제목\"])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "df.to_csv(f'../data/{query}_{start_date}~{end_date}_{num_pages}page.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
